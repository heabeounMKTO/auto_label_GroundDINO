{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd9d339-7716-4e45-a29b-298a338a50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dino_finder import DinoFinder\n",
    "import os \n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa995f1-edf4-46f2-92bb-931d76cd34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "WEIGHTS_PATH = os.path.join(\"weights\", \"groundingdino_swint_ogc.pth\")\n",
    "SAM_WEIGHTS = \"weights/sam_vit_h_4b8939.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa34c608-ae88-461e-9b6f-0369a1ab8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils \n",
    "def real_coords(x, img):\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[0]\n",
    "    y[0] = w * (x[0] - x[2] / 2)  # top left x\n",
    "    y[1] = h * (x[1] - x[3] / 2)  # top left y\n",
    "    y[2] = w * (x[0] + x[2] / 2)  # bottom right x\n",
    "    y[3] = h * (x[1] + x[3] / 2)  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def create_mask_collage(box_arr, cls_arr, img):\n",
    "    collages = []\n",
    "    imgc = []\n",
    "    col = Image.new(\"RGBA\", (img.shape[1], img.shape[0]))\n",
    "    for box in box_arr:\n",
    "        pil_img, coord, _crop_img = crop_image(box, img)\n",
    "        col.paste(pil_img, coord)\n",
    "    return col\n",
    "\n",
    "def show_mask(mask,  random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    return mask_image\n",
    "\n",
    "def crop_image(box, img):\n",
    "    rc = real_coords(np.array(box), img).tolist()\n",
    "    rc = [int(x) for x in rc]\n",
    "    crop_img = img[rc[1] : rc[3], rc[0] : rc[2]]\n",
    "    pil_img = Image.fromarray(crop_img)\n",
    "    return pil_img, (rc[0],rc[1]),crop_img\n",
    "\n",
    "def filter_result_by_prompt(prompt, img):\n",
    "    binary_img = None\n",
    "    all_binary_mask = np.zeros([img.shape[0], img.shape[1]], dtype=np.uint8) \n",
    "    for idx, box in enumerate(finder.boxes):\n",
    "        if finder.cls[idx] == prompt:\n",
    "            pil_img, coord, crop_img = crop_image(np.array(box), img) \n",
    "            sam_res, scores, logit = masker.sam_pred_from_img(img, np.array([int(x) for x in real_coords(np.array(box), img)]))\n",
    "            detections = sv.Detections(\n",
    "                xyxy=sv.mask_to_xyxy(masks=sam_res),\n",
    "                mask=sam_res\n",
    "            )\n",
    "            detections = detections[detections.area == np.max(detections.area)]\n",
    "            anno_img = mask_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "            highest_score_idx = int(np.where(scores == np.max(scores))[0])\n",
    "            binary_img = np.array(sam_res[highest_score_idx], dtype=np.uint8)\n",
    "            binary_img[binary_img > 0] = 255\n",
    "            all_binary_mask += binary_img        \n",
    "    masked_img = cv2.bitwise_and(img, img, mask=all_binary_mask) \n",
    "    return masked_img, all_binary_mask \n",
    "\n",
    "def find_from_prompt(image_path, prompt, confidence, text_confidence):\n",
    "    print(f\"PROOMPT : {prompt, type(prompt)}\")\n",
    "    boxes, conf, cls, an = finder.find_by_prompt(\n",
    "        prompt=prompt,\n",
    "        bt=(confidence / 100),\n",
    "        tt=(text_confidence / 100),\n",
    "        image_path=image_path,\n",
    "    )\n",
    "    an = cv2.cvtColor(an, cv2.COLOR_BGR2RGB)\n",
    "    collage = create_mask_collage(boxes, cls, image_path)\n",
    "    return an, collage\n",
    "\n",
    "\n",
    "def find_from_image(image_pth, prompt, bt, tt):\n",
    "    text_prompt = prompt\n",
    "    bt = bt\n",
    "    tt = tt\n",
    "    imgsrc, img = load_image(image_pth)\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model, image=img, caption=text_prompt, box_threshold=bt, text_threshold=tt\n",
    "    )\n",
    "    annotated_image = annotate(imgsrc, boxes, logits, phrases)\n",
    "    cv2.imwrite(f\"results/{os.path.basename(image_pth)}.png\", annotated_image)\n",
    "    return boxes.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44836185-2509-46b1-86a4-a51c5988301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/auto_label_GroundDINO/env/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/auto_label_GroundDINO/env/lib/python3.11/site-packages/transformers/modeling_utils.py:905: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/hb/auto_label_GroundDINO/env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/hb/auto_label_GroundDINO/env/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5038403868675232, 0.4861435890197754, 0.44026246666908264, 0.6965095400810242]]\n"
     ]
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "img = \"/home/hb/Pictures/test_female.png\"\n",
    "result = find_from_image(image_pth=img, prompt=\"human head\", bt =0.5, tt =0.22)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
